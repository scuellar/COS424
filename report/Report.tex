\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{multicol}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}[1]{E\left[ #1 \right]}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Project Report \\ \vspace{5mm} \large{COS 424: Interacting with Data}}
\author{Jonathan Balkind -- Santiago Cu\'ellar -- Jos\'e Ferreira -- Alex Tarr} 
\maketitle

\begin{multicols}{2}

% Section 1. Introduction
\section{Introduction} 

This problem is very important because the data sucks and therefore Yelp is desperate.


% Section 2. Factorization
\section{Factorization}

Matrix factorization (MF) methods have been extensively used in recommender systems. The idea is that we can characterize users and businesses by a feature vector in the same (relatively) low-dimensional latent space. Ideally, the features would represent meaningful dimensions that determine the characteristics of a business or user. However, matrix factorization techniques usually result in uninterpretable features. 

Nevertheless, in some sense, they perform operations akin to a singular value decomposition (SVDs) with the added advantages that it does not require a dense matrix , and that it is substantially less computationally expensive. SVD would require one to guess the missing values (inputation), as well as adding great computational expense to the problem. We next present the models used to assess the applicability of matrix factorization to the problem at hand (more details can be found in the Technical Appendix).

\paragraph{Model 1.} In sum, we want to factorize the very large sparse ratings matrix of users \emph{vs.} businesses into two oblong matrices $\mbf U\in\mathbb{R}^{n_u\times f}$ and $\mbf B\in\mathbb{R}^{n_b\times f}$, where $f$ is the number of latent factors. The important point is that we only wish to consider the ratings that we do have (thus ignoring all the unknown ratings). Therefore, we wish to find $\mbf U$ and $\mbf B$ that solve the following optimization problem:
\begin{equation}
\min_{\mbf U,\mbf B} = \sum_{(i,j)\in K} (r_{ij}-\mbf u_i^T\mbf b_j)^2+\lambda\left(||\mbf u_i||^2+||\mbf b_j||^2\right)
\end{equation}
where $K$ is the set of non-empty ratings in the matrix and the $\mbf u_i$ and $\mbf b_i$ are feature vectors associated with user $i$ and business $j$, respectively (columns in $\mbf U$ and $\mbf B$). The regularization parameter helps prevent overfitting (in a sense, accounting for the prior belief that most ratings are somewhat fair). With this model, we can easily predict a new rating using:
\begin{equation}
\hat r_{ij} = \mbf u_i^T\mbf b_j.
\end{equation}

\paragraph{Model 2.} Unfortunately, model 1 is rather restrictive. In particular, it doesn't account for user and business biases which one would expect to play a big role in defining the true ratings. As such, we consider a new prediction model:
\begin{equation}
\hat r_{ij} = \mu + p_i + q_j + \mbf u_i^T\mbf b_j
\end{equation}
where $\mbf p$ and $\mbf q$ represent average user and business biases, respectively, and $\mu$ is the global average rating. The optimization problem to be solved changes accordingly, and is now over $\mbf U$, $\mbf B$, $\mbf p$ and $\mbf q$ (see Technical Appendix for details).

\paragraph{Model 3.} Another model one can consider stems from the realization that business average ratings tend to represent a better baseline than user ratings in the Yelp dataset (as there is a large number of users that have a small amount of ratings). As such, it is helpful to consider a prediction model of the following form:
\begin{equation}
\hat r_{ij} =  p_i + q_j + \mbf u_i^T\mbf b_j
\end{equation}
where $q_j$ now represents the average rating for business $j$ and $p_i$ is the average rating offset for user $i$. Note that in this model we are only optimizing over the matrices $\mbf U$ and $\mbf B$. 

\paragraph{Model BL.} In order to directly assess the effectiveness of the matrix factorization technique, we compared the results against a baseline model using Model 3's framework without the factorized matrices. This requires no optimization.

% subsection on error analysis
\subsection{Results and Error Analysis}
The parameters $\lambda$ and the number of latent factors were chosen through 10-fold cross-validation. Two metrics were considering for assessing the quality of the model: root mean squared error (RMSE) and mean absolute error (MAE). The former has been the metric of choice for recommender systems at large, but we found that the latter also provides interesting insight into the performance of the models. 

The results obtained for each of the models can be seen in Table XYZXYZXYZXYZ below. We note that the factorized models fared worse than the simple baseline model. Below, we address some of the reasons that may explain these results.

\paragraph{Sparsity.} While the popularity of MF methods came about in the context of sparse ratings matrices, we suspect that excessive sparsity has substantial negative impact on the performance of the model. In particular, 83\% of the users in the training set have less than 5 reviews to their name and 49\% have a single review. The text associated with these reviews is, more often than not, descriptive of either an excellent experience or a terrible one. It appears that many of the users have created accounts solely for the purpose of writing a single extreme review, the effect of which is not accurately portrayed by our models.

\paragraph{Critic users.} We took the vector $\mbf p$ of average user offsets (from Model 3) and used it to split the users into three groups: positive, negative and fair. We then used the partitioned ratings matrix to fit three factorization models using Model 3. The cluster information and results obtained can be found in TABLE XYZXYZXYZXYZ. We note that the average error did not change much for fair users. However, positive users saw a substantial improvement in MAE. This is likely a result of the positive bias in the ratings (many ratings are 5) and the fact that our prediction model clips the ratings to the interval $[0,5]$. More worrying, however, was the decrease in performance for negative users. Unlike with the positive users, this does not appear to result from clipping, but rather from intrinsic variance in the ratings from these users. This suggests that it would be useful to employ different levels of regularization depending on the average offset of the user (future work).

\paragraph{Businesses.} Interestingly, the number of ratings businesses have does not appear to correlate positively with prediction accuracy (in the extremes of performance). We considered businesses with high accuracy predictions (RMSE$\leq$0.5) and those with low (RMSE$\geq$1.5). The set of low accuracy businesses had, on average, about 18 reviews, while the high accuracy set had about 13. However, we did see the same pattern as in users in the sense that the high accuracy set had a high average rating ($\approx4.1$) while the low average set had a lower rating ($\approx3.2$).

\subsection{Imputation}  
Given Model 3 proved to be the most successful of the three MF models considered, we attempted to better account for priors by filling in random ratings for users with 5 or less reviews (until they totalled 5 reviews). The filled-in values were just the business averages. Fitting Model 3 to this new ratings matrix resulted in modest improvements (see TABLE XYZXYZXYZ), and it is possible that picking the number of reviews to fill in more carefully could yield greater improvements. 


\newpage
\section{Technical Appendix}
This section provides a few additional details on the matrix factorization technique. 

The solution of each of the optimization problems follows from simple gradient descent (learning rate parameter $\gamma$). We consider each model in turn and present the optimization problem to be solved as well as the update equation that results.

\paragraph{Model 1} The optimization problem for this model is:
\begin{equation*}
\min_{\mbf U,\mbf B} = \sum_{(i,j)\in K} (r_{ij}-\mbf u_i^T\mbf b_j)^2+\lambda\left(||\mbf u_i||^2+||\mbf b_j||^2\right)
\end{equation*}
and results in the following simple update equations
\begin{equation*}
\mbf u_i \leftarrow \mbf u_i+\gamma\left(e_{ij}\mbf b_j-\lambda\mbf u_i\right)
\end{equation*}
\begin{equation*}
\mbf b_j \leftarrow \mbf b_j+\gamma\left(e_{ij}\mbf u_i-\lambda\mbf b_j\right).
\end{equation*}
where $e_{ij}\equiv r_{ij}-\hat r_{ij}$.

\paragraph{Model 2} The optimization problem is:
\begin{multline*}
\min_{\mbf U,\mbf B,\mbf p,\mbf q} = \sum_{(i,j)\in K} (r_{ij}-\mbf u_i^T\mbf b_j-p_i-q_j-\mu)^2\\
+\lambda\left(||\mbf u_i||^2+||\mbf b_j||^2+p_i+q_j\right)
\end{multline*}
so that we have two additional update equations:
\begin{equation*}
p_i\leftarrow p_i+\gamma\left(e_{ij}-\lambda p_i\right)
\end{equation*}
\begin{equation*}
q_j\leftarrow q_j+\gamma\left(e_{ij}-\lambda q_j\right).
\end{equation*}

\paragraph{Model 3} The optimization problem is:
\begin{multline*}
\min_{\mbf U,\mbf B} = \sum_{(i,j)\in K} (r_{ij}-\mbf u_i^T\mbf b_j-p_i-q_j)^2+\\
\lambda\left(||\mbf u_i||^2+||\mbf b_j||^2\right).
\end{multline*}
The update equations are similar to the ones for Model 1. We note that to estimate $\mbf p$ and $\mbf q$ from the data, we should account for priors. This prevents attributing excessive importance to individual ratings when the users (or businesses) have a small number of ratings. Therefore, for business $j$, we obtain:
\begin{equation*}
q_j = \frac{R \mu+\sum_i r_{ij}}{R+\sum_i\mbf 1\{r_{ij}\neq 0\}}
\end{equation*}
where $R$ is the ratio between the overall variance and the variance of the business mean ratings. The same technique is employed for estimating $\mbf q$.


\end{multicols}
\end{document}